{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, gensim,random\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from stop_words import get_stop_words\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from nltk.probability  import FreqDist\n",
    "import datetime\n",
    "import sklearn\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Based Filtering using LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(text):\n",
    "    \n",
    "    #word_tokenizing\n",
    "    global sent_to_words\n",
    "\n",
    "    def sent_to_words(sentences):\n",
    "        for sentence in sentences:\n",
    "            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "    data_words = list(sent_to_words(data))\n",
    "\n",
    "\n",
    "    p_stemmer = PorterStemmer()\n",
    "    en_stop = get_stop_words('en')\n",
    "\n",
    "    data_lemmatized = []\n",
    "\n",
    "    for i in data_words:\n",
    "        tokens = i\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "        data_lemmatized.append(' '.join(stemmed_tokens))\n",
    "    \n",
    "    global vectorizer,data_vectorized,lda_model,lda_output,best_lda_model\n",
    "    \n",
    "    if training == 1:\n",
    "        vectorizer = CountVectorizer(analyzer='word',       \n",
    "                                 #min_df=10,                        # minimum reqd occurences of a word \n",
    "                                 stop_words='english',             # remove stop words\n",
    "                                 lowercase=True,                   # convert all words to lowercase\n",
    "                                 token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                                 # max_features=30000,             # max number of uniq words\n",
    "                                )\n",
    "\n",
    "        data_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
    "\n",
    "        #Building LDA model\n",
    "        lda_model = LatentDirichletAllocation(n_components=8,               # Number of topics\n",
    "                                          max_iter=20,               # Max learning iterations\n",
    "                                          learning_method='online',   \n",
    "                                          random_state=100,          # Random state\n",
    "                                          batch_size=2,            # n docs in each learning iter\n",
    "                                          evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                          n_jobs = -1,               # Use all available CPUs\n",
    "                                         )\n",
    "        lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "\n",
    "        search_params = {'n_components': [3,5,7,9], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "        # Init the Model\n",
    "        lda = LatentDirichletAllocation()\n",
    "\n",
    "        # Init Grid Search Class\n",
    "        model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "        # Do the Grid Search\n",
    "        model.fit(data_vectorized)\n",
    "\n",
    "        # Printing params for best model among all the generated ones\n",
    "        # Best Model\n",
    "        best_lda_model = model.best_estimator_\n",
    "        \n",
    "        outfile = open('vectorizer.pickled','wb')\n",
    "        pickle.dump(vectorizer,outfile)\n",
    "        outfile.close()\n",
    "        outfile = open('data_vectorized.pickled','wb')\n",
    "        pickle.dump(data_vectorized,outfile)\n",
    "        outfile.close()\n",
    "        outfile = open('lda_output.pickled','wb')\n",
    "        pickle.dump(lda_output,outfile)\n",
    "        outfile.close()\n",
    "        outfile = open('lda_model.pickled','wb')\n",
    "        pickle.dump(lda_model,outfile)\n",
    "        outfile.close()\n",
    "        outfile = open('best_lda_model.pickled','wb')\n",
    "        pickle.dump(best_lda_model,outfile)\n",
    "        outfile.close()\n",
    "        \n",
    "    else :\n",
    "        \n",
    "        infile = open('vectorizer.pickled','rb')\n",
    "        vectorizer = pickle.load(infile)\n",
    "        infile.close()\n",
    "        infile = open('data_vectorized.pickled','rb')\n",
    "        data_vectorized = pickle.load(infile)\n",
    "        infile.close()\n",
    "        infile = open('lda_output.pickled','rb')\n",
    "        lda_output = pickle.load(infile)\n",
    "        infile.close()\n",
    "        infile = open('lda_model.pickled','rb')\n",
    "        lda_model = pickle.load(infile)\n",
    "        infile.close()\n",
    "        infile = open('best_lda_model.pickled','rb')\n",
    "        best_lda_model = pickle.load(infile)\n",
    "        infile.close()\n",
    "\n",
    "\n",
    "    #dominant topic in each doc\n",
    "\n",
    "    # Create Document - Topic Matrix\n",
    "    lda_output = best_lda_model.transform(data_vectorized)\n",
    "\n",
    "    # column names\n",
    "    topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "\n",
    "    # index names\n",
    "    docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "\n",
    "    # Make the pandas dataframe\n",
    "    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "    # Get dominant topic for each document\n",
    "    dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "    df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "    df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "\n",
    "    # defining topic keywords \n",
    "    # Topic-Keyword Matrix\n",
    "    df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "\n",
    "    # Assign Column and Index\n",
    "    df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "    df_topic_keywords.index = topicnames\n",
    "\n",
    "    # View\n",
    "    df_topic_keywords.head()\n",
    "\n",
    "    #get top 15 keywords for each doc\n",
    "\n",
    "\n",
    "    # Show top n keywords for each topic\n",
    "    def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "        keywords = np.array(vectorizer.get_feature_names())\n",
    "        topic_keywords = []\n",
    "        for topic_weights in lda_model.components_:\n",
    "            top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "            topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "        return topic_keywords\n",
    "\n",
    "    topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)  \n",
    "\n",
    "    #Given a piece of text, predicting the topic in document\n",
    "\n",
    "    def predict_topic(text):\n",
    "        global sent_to_words\n",
    "\n",
    "        mytext_2 = list(sent_to_words(text))\n",
    "        #print(mytext_2)\n",
    "\n",
    "        mytext_3 =[]\n",
    "\n",
    "        for i in mytext_2 :\n",
    "\n",
    "            tokens=i\n",
    "            stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "            #print(stopped_tokens)\n",
    "            stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "            #print(stemmed_tokens)\n",
    "            mytext_3.append(' '.join(stemmed_tokens))\n",
    "            #print(mytext_3)\n",
    "\n",
    "            mytext_4 = vectorizer.transform(mytext_3)\n",
    "\n",
    "        topic_probability_scores = best_lda_model.transform(mytext_4)\n",
    "        topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), :].values.tolist()\n",
    "        return topic, topic_probability_scores\n",
    "\n",
    "\n",
    "\n",
    "    #Given a piece of Text, predicting the documents that are related to it most closely\n",
    "\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "    def similar_documents(text, doc_topic_probs, documents = data, top_n=2, verbose=False):\n",
    "        topic, x  = predict_topic(text)\n",
    "        dists = euclidean_distances(x.reshape(1, -1), doc_topic_probs)[0]\n",
    "        doc_ids = np.argsort(dists)[:top_n]\n",
    "        return doc_ids, np.take(documents, doc_ids)\n",
    "\n",
    "    arr=[]\n",
    "    arr.append(text)\n",
    "    doc_ids, docs = similar_documents(text=arr, doc_topic_probs=lda_output, documents = data, top_n=2, verbose=True)\n",
    "    result_api.append(docs[0])\n",
    "    #print(result_api)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content based Filtering Using TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(text):\n",
    "    stopwords_en = get_stop_words('en')\n",
    "    #stop words of english are removed by the below function\n",
    "    def preprocessing(raw):\n",
    "        wordlist=nltk.word_tokenize(raw)\n",
    "        text=[w.lower() for w in wordlist if w not in stopwords_en]\n",
    "        return text\n",
    "\n",
    "\n",
    "    similarity_scores = []\n",
    "    doc_number = []\n",
    "\n",
    "    #We need to find documents that are similar to sample_doc from the corpus built above - data .\n",
    "\n",
    "    sample_doc = text\n",
    "    word_set= {'ibm'}\n",
    "\n",
    "    for doc in data:\n",
    "        word_set=word_set.union(set(preprocessing(doc)))\n",
    "    word_set=word_set.union(set(preprocessing(sample_doc)))\n",
    "\n",
    "\n",
    "    i=0\n",
    "    for doc in data:\n",
    "        text1=preprocessing(doc)\n",
    "        text2=preprocessing(sample_doc)\n",
    "\n",
    "        #TF Calculations\n",
    "\n",
    "        freqd_text1=FreqDist(text1)\n",
    "        text1_length=len(text1)\n",
    "\n",
    "        text1_tf_dict = dict.fromkeys(word_set,0)\n",
    "        for word in text1:\n",
    "            text1_tf_dict[word] = freqd_text1[word]/text1_length\n",
    "\n",
    "\n",
    "        freqd_text2=FreqDist(text2)\n",
    "        text2_length=len(text2)\n",
    "\n",
    "        text2_tf_dict = dict.fromkeys(word_set,0)\n",
    "        for word in text2:\n",
    "            text2_tf_dict[word] = freqd_text2[word]/text2_length\n",
    "\n",
    "\n",
    "        #IDF Calculations\n",
    "\n",
    "        text12_idf_dict=dict.fromkeys(word_set,0)\n",
    "        text12_length = len(data)\n",
    "        for word in text12_idf_dict.keys():\n",
    "            if word in text1:\n",
    "                text12_idf_dict[word]+=1\n",
    "            if word in text2:\n",
    "                text12_idf_dict[word]+=1\n",
    "\n",
    "        import math\n",
    "        for word,val  in text12_idf_dict.items():\n",
    "            if val == 0 :\n",
    "                val=0.01\n",
    "                text12_idf_dict[word]=1+math.log(text12_length/(float(val)))\n",
    "\n",
    "\n",
    "        #TF-IDF Calculations\n",
    "\n",
    "        text1_tfidf_dict=dict.fromkeys(word_set,0)\n",
    "        for word in text1:\n",
    "            text1_tfidf_dict[word] = (text1_tf_dict[word])*(text12_idf_dict[word])\n",
    "\n",
    "        text2_tfidf_dict=dict.fromkeys(word_set,0)\n",
    "        for word in text2:\n",
    "            text2_tfidf_dict[word] = (text2_tf_dict[word])*(text12_idf_dict[word])\n",
    "\n",
    "\n",
    "        #Finding cosine distance which ranges between 0 and 1. 1 implies documents are similar since cos-inverse(0)=1 that is \n",
    "        #vectors are collinear.cos-inverse(90)=1 that is vectors are othogonal to each other implying compltely dissimilar.\n",
    "\n",
    "        v1=list(text1_tfidf_dict.values())\n",
    "        v2=list(text2_tfidf_dict.values())\n",
    "\n",
    "        similarity= 1- nltk.cluster.cosine_distance(v1,v2)\n",
    "        doc_number.append(int(i))\n",
    "        similarity_scores.append(float(format(similarity*100,'4.2f')))\n",
    "        i=i+1\n",
    "\n",
    "        #print(\"Similarity Index = {:4.2f} % \".format(similarity*100))\n",
    "\n",
    "    #print('Document IDs : ' + ', '.join(str(e) for e in doc_number))    \n",
    "    #print('Similarity % : ' + ', '.join(str(e) for e in similarity_scores))\n",
    "    \n",
    "    #Based on similarity scores computed previously sort the document indices in ascending leading to most similar document indices\n",
    "    #present at the end of array\n",
    "    sorted_doc_list = [doc_number for _,doc_number in sorted(zip(similarity_scores,doc_number))]\n",
    "\n",
    "\n",
    "    #printing top 3 documents which are most similar to sample_doc\n",
    "    j = 0\n",
    "    n=4\n",
    "    for doc in reversed(sorted_doc_list):\n",
    "        #print('\\n\\n',data[doc][:40])\n",
    "        result_api.append(data[doc][0:])\n",
    "        j=j+1\n",
    "        if j==n :\n",
    "            break\n",
    "            \n",
    "    #print(result_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles Recommended: ['Tip to deal with Fear :“I don’t want to do it,” I said to the ropes course instructor.  “That’s your call, I’m not into pressuring people into things they aren’t comfortable with,” he said back.  I sat down; almost relieved I didn’t have to face my fears. I am terrified of ropes courses, especially of heights and climbing in the trees. I’ve grown up afraid of heights, I’ve always been afraid of rock climbing, even though I’m harnessed in. Being harnessed in this ropes course didn’t make me feel any better; the thought of being in the air absolutely filled every inch of my body with fear.  But then there was the peer pressure from my teammates and coaches to do it, so I decided today was a good day to face my fears. I was surrounded by a group of people closets to me so I should just do it.  I built the up courage, knowing that if I didn’t like it I could get down an escape root. It also helped knowing I wasn’t the only one who was afraid in the group.  I started on the easiest course of the easy courses. I was TERRIFIED like I didn’t think I could lift one foot in front of the other. The only thing that kept me moving forward was that I was holding everyone up behind me. I kept going, right foot then left foot and before I knew it I was on a platform again, telling myself that wasn’t so bad. I’d look at the next challenge and have to talk myself through it again.  Once I reached the end I didn’t think the easiest level was too bad, so I went up to the yellow the level, the next difficulty level. I did the same thing as the easiest level; I talked myself through it telling myself one step at a time I would make it to the next platform.', 'Tip to Overcome Anger :Know when to seek help Learning to control anger is a challenge for everyone at times. Seek help for anger issues if your anger seems out of control, causes you to do things you regret or hurts those around you.', 'Tip to Overcome Anger : Get some exercise Physical activity can help reduce stress that can cause you to become angry. If you feel your anger escalating, go for a brisk walk or run, or spend some time doing other enjoyable physical activities.', \"Tip to Overcome Anger : Take a timeout Timeouts aren't just for kids. Give yourself short breaks during times of the day that tend to be stressful. A few moments of quiet time might help you feel better prepared to handle what's ahead without getting irritated or angry.\", 'Tip to Overcome Sadness : Know what causes depression. Depression has many causes, and researchers are not yet entirely certain how it works. Early trauma may cause changes in how your brain handles fear and stress. Many studies suggest that clinical depression may be partly genetic. Life changes such as losing a loved one or going through a divorce may trigger an episode of major depression.[48] Clinical depression is a complex disorder. It may be partially caused by trouble with neurotransmitters in your brain, such as serotonin and dopamine.[49] Medication may help regulate these chemicals and alleviate depression. Substance abuse, such as misuse of alcohol and drug use, is strongly linked to depression.[50] Studies suggest that lesbian, gay, and bisexual individuals may suffer from higher rates of depression. This may be because of a lack of social and personal support systems']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df=pd.read_excel(\"C:\\\\Users\\\\Amitabh Tiwari\\\\ibm-hack\\\\articles.xlsx\")\n",
    "data=[]\n",
    "for v in df['content']:\n",
    "    data.append(v)\n",
    "\n",
    "#Getting user statistics\n",
    "text=\"I am feeling angry. How to overcome Anger\"\n",
    "training=0\n",
    "\n",
    "\n",
    "#result_api will contain article ids of articles that are to be recommended\n",
    "result_api=[]\n",
    "\n",
    "tfidf(text)\n",
    "lda(text)\n",
    "\n",
    "\n",
    "#print(\"Articles Recommended: {}\".format(result_api))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
